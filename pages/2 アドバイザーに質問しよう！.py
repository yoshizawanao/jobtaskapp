import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.memory import ConversationBufferWindowMemory
from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate
from langchain_core.runnables import RunnableConfig
from langchain_community.callbacks import StreamlitCallbackHandler

# models
from langchain_openai import ChatOpenAI

# custom tools
from tools.search_ddg import search_ddg
from tools.fetch_page import fetch_page

import os

os.environ["LANGCHAIN_TRACING"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "jobtaskapp"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "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"

def init_page():
    st.set_page_config(
        page_title="Ask My PDF(s)",
        page_icon="ğŸ§"
    )
    st.sidebar.title("Options")

def select_model():
    temperature= st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.0,step=0.01
    )
    models = ("GPT-4", "GPT-3.5 (not recommended)")
    model = st.sidebar.radio("Choose a model:", models)
    if model == "GPT-3.5 (not recommended)":
        return ChatOpenAI(
            temperature=temperature, model_name="gpt-3.5-turbo")
    elif model == "GPT-4":
        return ChatOpenAI(
            temperature=temperature, model_name="gpt-4o")



def init_qa_chain(llm):
    llm = llm
    # llm = ChatOpenAI(temperature=0, model_name="gpt-4o")
    prompt = ChatPromptTemplate.from_template("""
    ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ±‚ã‚ã‚‹æ¡ä»¶ã«ã‚ã£ãŸè¦³å…‰åœ°ã‚’ææ¡ˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
    ä»¥ä¸‹ã®å‰æçŸ¥è­˜(è¦³å…‰åœ°ã®ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆ)ã‚’ç”¨ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚

    ===
    å‰æçŸ¥è­˜
    {context}

    ===
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•
    {question}
    """)
    retriever = st.session_state.vectorstore.as_retriever(
       
        search_type="similarity",

        search_kwargs={"k":10}
    )
    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain





CUSTOM_SYSTEM_PROMPT2 = """
ã‚ãªãŸã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«åŸºã¥ã„ãŸè¦³å…‰åœ°ã‚’ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã§èª¿ã¹æä¾›ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€èª¿æŸ»ã—ãŸæƒ…å ±ã‚’èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
æ—¢ã«çŸ¥ã£ã¦ã„ã‚‹ã“ã¨ã ã‘ã«åŸºã¥ã„ã¦ç­”ãˆãªã„ã§ãã ã•ã„ã€‚å›ç­”ã™ã‚‹å‰ã«ã§ãã‚‹é™ã‚Šæ¤œç´¢ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
(ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒèª­ã‚€ãƒšãƒ¼ã‚¸ã‚’æŒ‡å®šã™ã‚‹ãªã©ã€ç‰¹åˆ¥ãªå ´åˆã¯ã€æ¤œç´¢ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚)

æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ã‚’è¦‹ãŸã ã‘ã§ã¯æƒ…å ±ãŒã‚ã¾ã‚Šãªã„ã¨æ€ã‚ã‚Œã‚‹å ´åˆã¯ã€æ¬¡ã®2ã¤ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¤œè¨ã—ã¦è©¦ã—ã¦ã¿ã¦ãã ã•ã„ã€‚

- æ¤œç´¢çµæœã®ãƒªãƒ³ã‚¯ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€å„ãƒšãƒ¼ã‚¸ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã€èª­ã‚“ã§ã¿ã¦ãã ã•ã„ã€‚
- 1ãƒšãƒ¼ã‚¸ãŒé•·ã™ãã‚‹å ´åˆã¯ã€3å›ä»¥ä¸Šãƒšãƒ¼ã‚¸é€ã‚Šã—ãªã„ã§ãã ã•ã„ï¼ˆãƒ¡ãƒ¢ãƒªã®è² è·ãŒã‹ã‹ã‚‹ãŸã‚ï¼‰ã€‚
- æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’å¤‰æ›´ã—ã¦ã€æ–°ã—ã„æ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
- æ¤œç´¢ã™ã‚‹å†…å®¹ã«å¿œã˜ã¦æ¤œç´¢ã«åˆ©ç”¨ã™ã‚‹è¨€èªã‚’é©åˆ‡ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
  - ä¾‹ãˆã°ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°é–¢é€£ã®è³ªå•ã«ã¤ã„ã¦ã¯è‹±èªã§æ¤œç´¢ã™ã‚‹ã®ãŒã„ã„ã§ã—ã‚‡ã†ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯éå¸¸ã«å¿™ã—ãã€ã‚ãªãŸã»ã©è‡ªç”±ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
ãã®ãŸã‚ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åŠ´åŠ›ã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã«ã€ç›´æ¥çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

=== æ‚ªã„å›ç­”ã®ä¾‹ ===
- ã“ã‚Œã‚‰ã®ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
- ã“ã‚Œã‚‰ã®ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ã‚³ãƒ¼ãƒ‰ã‚’æ›¸ãã“ã¨ãŒã§ãã¾ã™ã€‚
- æ¬¡ã®ãƒšãƒ¼ã‚¸ãŒå½¹ç«‹ã¤ã§ã—ã‚‡ã†ã€‚

=== è‰¯ã„å›ç­”ã®ä¾‹ ===
- ã“ã‚Œã¯ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã§ã™ã€‚ -- ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’ã“ã“ã« --
- ã‚ãªãŸã®è³ªå•ã®ç­”ãˆã¯ -- å›ç­”ã‚’ã“ã“ã« --

å›ç­”ã®æœ€å¾Œã«ã¯ã€å‚ç…§ã—ãŸãƒšãƒ¼ã‚¸ã®URLã‚’**å¿…ãš**è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚ï¼ˆã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯å›ç­”ã‚’æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼‰

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä½¿ç”¨ã—ã¦ã„ã‚‹è¨€èªã§å›ç­”ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ—¥æœ¬èªã§è³ªå•ã—ãŸå ´åˆã¯ã€æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¹ãƒšã‚¤ãƒ³èªã§è³ªå•ã—ãŸå ´åˆã¯ã€ã‚¹ãƒšã‚¤ãƒ³èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""




def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
    if clear_button or "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant", "content": "ã“ã‚“ã«ã¡ã¯ï¼ãªã‚“ã§ã‚‚è³ªå•ã‚’ã©ã†ãï¼"}
        ]
        st.session_state['memory'] = ConversationBufferWindowMemory(
            return_messages=True,
            memory_key="chat_history",
            k=10
        )
    
def page_ask_my_pdf(llm, prompt):
    chain = init_qa_chain(llm)
    # st.markdown("## Answer")
    # answer_container = st.empty()
    full_response = chain.invoke(prompt)
    
    st.session_state['memory'].save_context(
        {"input": prompt + "(ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆå‚ç…§)"},
        {"output": full_response}
    )   
    return full_response

def create_agent(llm):
    tools = [search_ddg, fetch_page]
    prompt = ChatPromptTemplate.from_messages([
        ("system", CUSTOM_SYSTEM_PROMPT2),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad")
    ])
    llm = llm
    # llm = ChatOpenAI(temperature=0, model_name="gpt-4o")
    agent = create_tool_calling_agent(llm, tools, prompt)
    return AgentExecutor(
        agent=agent,
        tools=tools,
        verbose=True,
        memory=st.session_state['memory']
    )



def main():
    init_page()
    init_messages()
    llm=select_model()
    st.title("ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã«è³ªå•ã—ã‚ˆã†ï¼ ğŸ§")
    if "vectorstore" not in st.session_state:
        st.warning("ã¾ãšã¯ ğŸ“„ Upload PDF(s) ã‹ã‚‰PDFãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã­")
        return
    
    with st.chat_message("ai"):
        st.markdown("""
                    ã“ã‚“ã«ã¡ã¯ï¼ç§ã¯è¦³å…‰åœ°ã‚¢ãƒ‰ãƒã‚¤ã‚¶ãƒ¼ã§ã™ï¼  
                    ã‚ãªãŸãŒè¡ŒããŸã„è¦³å…‰åœ°ã®æ¡ä»¶ã‚’æ•™ãˆã¦ã„ãŸã ã‘ãŸã‚‰ã€  
                    1.ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆã‚’å‚ç…§ã—ã¦ææ¡ˆ(å±¥æ­´ã«(ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆå‚ç…§)ã¨è¡¨è¨˜ã•ã‚Œã‚‹ã‚ˆï¼)   
                    2.ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¤œç´¢ã‚ˆã‚Šææ¡ˆ  
                    ã®ï¼’ã¤ã®æ–¹æ³•ã§ã‚ãªãŸã«é©ã—ãŸè¦³å…‰åœ°ã‚’ææ¡ˆã•ã›ã¦ã„ãŸã ãã¾ã™ï¼  
                    é©åˆ‡ãªææ¡ˆã‚’ã™ã‚‹ãŸã‚ã«ã€å¿…ãšè³ªå•ã«ã¯[å ´æ‰€]ã‚’è¨˜å…¥ã—ã¦ãã ã•ã„ã€‚    
                    å‡ºæ¥ã‚‹ã ã‘ç´°ã‹ã„è³ªå•ã‚’ã—ã¦ãã‚Œã‚‹ã¨ã‚ˆã‚Šå…·ä½“çš„ãªææ¡ˆãŒå¯èƒ½ã§ã™ï¼  
                    """)
    
    web_browsing_agent = create_agent(llm)

    for msg in st.session_state['memory'].chat_memory.messages:
        st.chat_message(msg.type).write(msg.content)

    if prompt := st.chat_input(placeholder="æ¹˜å—ã®å®¶æ—é€£ã‚Œã«ãŠã™ã™ã‚ã®è¦³å…‰åœ°ã¯ï¼Ÿ"):
        st.chat_message("user").write(prompt + "(ãƒ‘ãƒ³ãƒ•ãƒ¬ãƒƒãƒˆå‚ç…§)")
        # page_ask_my_pdf(llm, prompt)

        with st.chat_message("assistant"):
            answer = page_ask_my_pdf(llm, prompt)
            st.write(answer)

        st.chat_message("user").write(prompt)
        
        with st.chat_message("assistant"):    
            # ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°ã®è¨­å®š (ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å‹•ä½œã®å¯è¦–åŒ–ç”¨)
            st_cb = StreamlitCallbackHandler(
                st.container(), expand_new_thoughts=True)

            # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œ
            response = web_browsing_agent.invoke(
                {'input': prompt},
                config=RunnableConfig({'callbacks': [st_cb]})
            )
            st.write(response["output"])
    



if __name__ == '__main__':
    main()